<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>消息中间件之Kafka | 春</title><meta name="author" content="zjyan"><meta name="copyright" content="zjyan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、引言1.起源Kafka是如何诞生的呢？ Kafka 是由 LinkedIn 公司开发的一种分布式、可扩展、高吞吐量的消息系统。它的设计灵感来自于 LinkedIn 在处理大规模实时数据处理和数据流的挑战中的经验和教训 在过去，LinkedIn 使用了一些传统的消息系统来处理数据流和实时数据处理，但是这些系统的性能和可扩展性无法满足 LinkedIn 处理大规模实时数据流的需求。为了解决这个问题">
<meta property="og:type" content="article">
<meta property="og:title" content="消息中间件之Kafka">
<meta property="og:url" content="http://yzj.life/2023/04/04/MQ%E4%B9%8BKafka/index.html">
<meta property="og:site_name" content="春">
<meta property="og:description" content="一、引言1.起源Kafka是如何诞生的呢？ Kafka 是由 LinkedIn 公司开发的一种分布式、可扩展、高吞吐量的消息系统。它的设计灵感来自于 LinkedIn 在处理大规模实时数据处理和数据流的挑战中的经验和教训 在过去，LinkedIn 使用了一些传统的消息系统来处理数据流和实时数据处理，但是这些系统的性能和可扩展性无法满足 LinkedIn 处理大规模实时数据流的需求。为了解决这个问题">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://static-2w2.pages.dev/black/10.jpeg">
<meta property="article:published_time" content="2023-04-04T11:45:59.000Z">
<meta property="article:author" content="zjyan">
<meta property="article:tag" content="MQ">
<meta property="article:tag" content="Kafka">
<meta property="article:tag" content="消息队列">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://static-2w2.pages.dev/black/10.jpeg"><link rel="shortcut icon" href="https://static-2w2.pages.dev/imgs/rabbit.png"><link rel="canonical" href="http://yzj.life/2023/04/04/MQ%E4%B9%8BKafka/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b792d399a45da5b085a5a91cd2117f9e";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '消息中间件之Kafka',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-04-15 21:48:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="iron-container iron-circle"><div class="iron-box1 iron-circle iron-center"></div><div class="iron-box2 iron-circle iron-center"></div><div class="iron-box3 iron-circle iron-center"></div><div class="iron-box4 iron-circle iron-center"></div><div class="iron-box5 iron-circle iron-center"></div><div class="iron-box6 iron-circle"><div class="iron-coil" style="--i: 0"></div><div class="iron-coil" style="--i: 1"></div><div class="iron-coil" style="--i: 2"></div><div class="iron-coil" style="--i: 3"></div><div class="iron-coil" style="--i: 4"></div><div class="iron-coil" style="--i: 5"></div><div class="iron-coil" style="--i: 6"></div><div class="iron-coil" style="--i: 7"></div></div></div></div><script async="async">const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
document.getElementById('loading-box').addEventListener('click',()=> {preloader.endLoading()})

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://static-2w2.pages.dev/imgs/cx.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://static-2w2.pages.dev/black/10.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">春</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">消息中间件之Kafka</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2023-04-04T11:45:59.000Z" title="发表于 2023-04-04 19:45:59">2023-04-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>45分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="消息中间件之Kafka"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h2><h3 id="1-起源"><a href="#1-起源" class="headerlink" title="1.起源"></a>1.起源</h3><p>Kafka是如何诞生的呢？</p>
<p>Kafka 是由 LinkedIn 公司开发的一种分布式、可扩展、高吞吐量的消息系统。它的设计灵感来自于 LinkedIn 在处理大规模实时数据处理和数据流的挑战中的经验和教训</p>
<p>在过去，LinkedIn 使用了一些传统的消息系统来处理数据流和实时数据处理，但是这些系统的性能和可扩展性无法满足 LinkedIn 处理大规模实时数据流的需求。为了解决这个问题，LinkedIn 在 2008 年开始设计和开发 Kafka</p>
<p>Kafka 的设计目标是支持高吞吐量、低延迟的数据流处理，同时能够处理海量的数据。Kafka 借鉴了消息队列的概念，但与传统的消息系统不同的是，Kafka 的设计重点放在了可扩展性和高吞吐量上。它使用了一些优秀的设计和算法来实现高效的消息传输和存储，比如消息分区和批量传输等</p>
<p>在 Kafka 的发展历程中，LinkedIn 逐步将它开源，并加入了更多的特性和功能，使得 Kafka 成为了一种非常受欢迎的分布式消息系统。现在 Kafka 已经成为了 Apache 基金会的顶级项目，被广泛应用于实时数据处理、流处理、日志处理等场景</p>
<h3 id="2-应用"><a href="#2-应用" class="headerlink" title="2.应用"></a>2.应用</h3><p>Kafka发展到如今，已经应用于诸多系统与领域中，那么大体上分类，它有哪些应用场景呢？</p>
<p>主要可以分为3类</p>
<ul>
<li>数据集成</li>
<li>大数据领域</li>
<li>流计算生成</li>
</ul>
<p>但是最基本的，是它可以作为消息队列使用</p>
<h4 id="2-1-消息传递-Messaging"><a href="#2-1-消息传递-Messaging" class="headerlink" title="2.1 消息传递 Messaging"></a>2.1 消息传递 Messaging</h4><p>消息传递就是发送数据，作为 TCP HTTP 或者 RPC 的替代方案，可以实现异步、 解耦、削峰</p>
<blockquote>
<p>RabbitMQ 和 RocketMQ 能做的事情，它也能做</p>
</blockquote>
<p>因为 Kafka 的吞吐量更高，在大规模消息系统中更有优势</p>
<p>第二个是大数据领域的使用，比如网站行为分析</p>
<h4 id="2-2-网站行为分析"><a href="#2-2-网站行为分析" class="headerlink" title="2.2 网站行为分析"></a>2.2 网站行为分析</h4><h5 id="2-2-1-Website-Activity-Tracking-网站活动跟踪"><a href="#2-2-1-Website-Activity-Tracking-网站活动跟踪" class="headerlink" title="2.2.1 Website Activity Tracking 网站活动跟踪"></a>2.2.1 Website Activity Tracking 网站活动跟踪</h5><p>把用户活动发布到数据管道中，可以用来做监控、实时处理、报表等等，比如社交网站的行为跟踪，购物网站的行为跟踪，这样可以实现更加精准的内容推荐</p>
<p>例:外卖、物流、电力系统的实时信息</p>
<h5 id="2-2-2-Log-Aggregation-日志聚合"><a href="#2-2-2-Log-Aggregation-日志聚合" class="headerlink" title="2.2.2 Log Aggregation 日志聚合"></a>2.2.2 Log Aggregation 日志聚合</h5><p>又比如用 Kafka 来实现日志聚合。这样就不用把日志记录到本地磁盘或者数据库，实现分布式的日志聚合</p>
<h5 id="2-2-3-Metrics-应用指标监控"><a href="#2-2-3-Metrics-应用指标监控" class="headerlink" title="2.2.3 Metrics 应用指标监控"></a>2.2.3 Metrics 应用指标监控</h5><p>还可以用来记录运营监控数据</p>
<p>例，对于贷款公司，需要监控贷款的业务数据：今天放出去多少笔贷款，放出去的总金额，用户的年龄分布、地区分布、性别分布等等</p>
<p>或者对于运维数据的监控，CPU、内存、磁盘、网络连接的使用情况，可以实现告警</p>
<h4 id="2-3-数据集成-流计算"><a href="#2-3-数据集成-流计算" class="headerlink" title="2.3 数据集成+流计算"></a>2.3 数据集成+流计算</h4><p>首先是数据集成，指的是把 Kafka 的数据导入 Hadoop、HBase 等离线数据仓库，实现数据分析</p>
<p>其次是<strong>流计算</strong>，什么是流（Stream）?</p>
<p>它不是静态的数据，而是没有边界的、 源源不断的产生的数据，就像水流一样。流计算指的就是 Stream 对做实时的计算。 Kafka 在 0.10 版本后，内置了流处理框架 API——Kafka Streams<br>所以，它跟 RabbitMQ 的定位差别还是比较大的，不仅仅是一个简单的消息中间件，而且是一个流处理平台</p>
<p>在 Kafka 里面，消息被称为日志，日志就是消息的数据文件</p>
<h3 id="3-Kafka和ZK的关系"><a href="#3-Kafka和ZK的关系" class="headerlink" title="3.Kafka和ZK的关系"></a>3.Kafka和ZK的关系</h3><p>在安装 Kafka 的时候，必须要依赖 ZK 的服务，在生产环境通常是 ZK 的集群，而且 Kafka 还自带了一个 ZK 服务，那么ZK 做了什么事情呢?</p>
<p>总结概括就是，利用 ZK 的有序节点、临时节点和监听机制，ZK 帮 Kafka 做了以下这些事情</p>
<p>配置中心(管理 Broker、Topic、Partition、Consumer 的信息，包括元数据的 变动)、负载均衡、命名服务、分布式通知、集群管理和选举、分布式锁</p>
<p>关于ZK，可以参考之前的文章<a href="https://yzj.life/2023/03/31/Zookeeper/">Zookeeper</a></p>
<h2 id="二、基础架构"><a href="#二、基础架构" class="headerlink" title="二、基础架构"></a>二、基础架构</h2><p>首先来一张架构图，看懂这张图就看懂了Kafka的架构</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka01.png" alt="在这里插入图片描述" style="zoom:38%;" />

<h3 id="1-Broker"><a href="#1-Broker" class="headerlink" title="1.Broker"></a>1.Broker</h3><img src="https://static-2w2.pages.dev/post/mq-kafka-broker.png" alt="image-20230413125501013" style="zoom:50%;" />

<p>Broker:Kafka 作为一个中间件，是帮我们存储和转发消息的，它做的事情就像 中介，所以 Kafka 的服务也叫做 Broker，默认是 9092 的端口。生产者和消费者都需 要跟这个 Broker 建立一个连接，才可以实现消息的收发</p>
<h3 id="2-消息"><a href="#2-消息" class="headerlink" title="2.消息"></a>2.消息</h3><p>客户端之间传输的数据叫做消息，或者叫做记录(Record 名词 [ˈrekɔːd ])</p>
<p>在客户端的代码中，Record 可以是一个 KV 键值对。 生产者对应的封装类是 ProducerRecord，消费者对应的封装类是 ConsumerRecord</p>
<p>消息在传输的过程中需要序列化，所以代码里面要指定序列化工具</p>
<blockquote>
<p>RabbitMQ是将消息序列化成二机制</p>
</blockquote>
<p>消息在服务端的存储格式<a target="_blank" rel="noopener" href="http://kafka.apache.org/documentation/#messageformat">(RecordBatch 和 Record)</a></p>
<h3 id="3-生产者"><a href="#3-生产者" class="headerlink" title="3.生产者"></a>3.生产者</h3><p>发送消息的一方叫做生产者，接收消息的一方叫做消费者</p>
<p>为了提升消息发送速率，生产者不是逐条发送消息给 Broker，而是批量发送的</p>
<p>多少条发送一次由一个参数决定</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pros.put(<span class="string">&quot;batch.size&quot;</span>,<span class="number">16384</span>);</span><br></pre></td></tr></table></figure>

<h3 id="4-消费者"><a href="#4-消费者" class="headerlink" title="4.消费者"></a>4.消费者</h3><p>一般来说消费者获取消息有两种模式，一种是 Pull 模式，一种是 Push 模式。</p>
<p>Pull 模式就是消费放在 Broker，消费者自己决定什么时候去获取。Push 模式是 消息放在 Consumer，只要有消息到达 Broker，都直接推给消费者。</p>
<p>RabbitMQ Consumer 及支持 Push 又支持 Pull，一般用的是 Push</p>
<p><strong>Kafka 只有 Pull 模式</strong></p>
<p>在 Push 模式下，如果消息产生速度远远大于消费者消费消息的速率，那消费者就会不堪重负(你已经吃不下了，但是还要不断地往你嘴里塞)，直到挂掉</p>
<p> 而且消费者可以自己控制一次到底获取多少条消息</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认 500 在 poll 方法里面可以指定</span></span><br><span class="line"><span class="attr">max.poll.records</span></span><br></pre></td></tr></table></figure>

<h3 id="5-Topic"><a href="#5-Topic" class="headerlink" title="5.Topic"></a>5.Topic</h3><p>生产者跟消费者是怎么关联起来的呢？或者说，生产者发送的消息，怎么才能到达某个特定的消费者？</p>
<p>他们要通过队列关联起来，也就是说</p>
<p><strong>生产者发送消息，要指定发给哪个队列</strong></p>
<p><strong>消费者接收消息，要指定从哪个队列接收</strong></p>
<p> 在 Kafka 里面，这个队列叫做 Topic，是一个逻辑的概念，可以理解为一组消息的集合（不同业务用途的消息）</p>
<p>生产者和 Topic 以及 Topic 和消费者的关系都是多对多，一个生产者可以发送消息到多个 Topic，一个消费者也可以从多个 Topic 获取消息(但是不建议这么做）</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-topic.png" alt="image-20230413130155422" style="zoom:35%;" />

<p>注意，生产者发送消息时，如果 Topic 不存在，会自动创建</p>
<p>由一个参数<code>auto.create.topics.enable</code>控制</p>
<p>默认为true，但是如果要彻底删掉一个 Topic，这个参数必须改成 false，否则只要有代码使用这个 Topic，它就会自动创建</p>
<h3 id="6-Partition-与-Cluster"><a href="#6-Partition-与-Cluster" class="headerlink" title="6.Partition 与 Cluster"></a>6.Partition 与 Cluster</h3><p>如果说一个 Topic 中的消息太多，会带来两个问题：</p>
<p>第一个是不方便横向扩展，比如我想要在集群中把数据分布在不同的机器上实现扩展，而不是通过升级硬件做到，如果一个 Topic 的消息无法在物理上拆分到多台机器的时候，这个是做不到的。 </p>
<p>第二个是并发或者负载的问题，所有的客户端操作的都是同一个 Topic，在高并发 的场景下性能会大大下降。</p>
<p>那么，怎么解决这个问题呢？我们想到的就是把一个 Topic 进行拆分—经典分片思想</p>
<p>Kafka 引入了一个分区(Partition)的概念，一个Topic可以划分成多个分区</p>
<p>分区在创建 Topic 的时候指定，每个 Topic 至少有一个分区</p>
<blockquote>
<p>Partition 思想上有点类似于分库分表，实现的也是<strong>横向扩展</strong>和<strong>负载</strong>的目的</p>
</blockquote>
<p>举个例子，Topic 有 3 个分区，生产者依次发送 9 条消息，对消息进行编号</p>
<p>第一个分区存 1 4 7，第二个分区存 2 5 8 ，第三个分区存 3 6 9，这个就实现了负载</p>
<p>每个 partition 都有一个物理目录</p>
<p>在配置的数据目录下(日志就是数据):：&#x2F;tmp&#x2F;kafka-logs&#x2F;</p>
<p>跟 RabbitMQ 不一样的地方是，Partition 里面的消息被<strong>读取</strong>之后<strong>不会被删除</strong>，所以同一批消息在一个 Partition 里面顺序、追加写入的，这个也是 Kafka 吞吐量大的一 个很重要的原因</p>
<h3 id="7-Partition-副本-Replica-机制"><a href="#7-Partition-副本-Replica-机制" class="headerlink" title="7.Partition 副本 Replica 机制"></a>7.Partition 副本 Replica 机制</h3><p>如果 Partition 的数据只存储一份，在发生网络或者硬件故障的时候，该分区的数据就无法访问或者无法恢复了</p>
<p>因此，Kafka 在 0.8 的版本之后增加了副本机制</p>
<p>每个 Partition 可以有若干个副本(Replica)，副本必须在<strong>不同的 Broker</strong> 上面。，一般我们说的副本包括其中的主节点</p>
<p>举例：部署了 3 个 Broker，该 Topic 有 3 个分区，每个分区一共 3 个副本</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-replica.png" alt="image-20230413131142867" style="zoom:25%;" />

<p>红色就是Leader，是生产者发送消息和消费者读取消息的对象</p>
<p>Follower的数据都是从leader同步的</p>
<blockquote>
<p>需要注意的是，只有Leader具有写入和消费能力，Follower不具有单独写入和消费能力</p>
</blockquote>
<h3 id="8-Segment"><a href="#8-Segment" class="headerlink" title="8.Segment"></a>8.Segment</h3><p>Kafka 的数据是放在后缀.log 的文件里面的，如果一个 Partition 只有一个 log 文件，消息不断地追加，这个 log 文件也会变得越来越大，这个时候要检索数据效率就很低了</p>
<p> 所以干脆把 Partition 再做一个切分，切分出来的单位就叫做段(Segment)</p>
<p>实际上Kafka 的存储文件是划分成段来存储的， 默认存储路径:&#x2F;tmp&#x2F;kafka-logs&#x2F;</p>
<p>每个 Segment 都有至少有 1 个数据文件和 2 个索引文件，这 3 个文件是成套出现的</p>
<p>默认一个Segment大小是1G</p>
<h3 id="9-Consumer-Group-消费者组"><a href="#9-Consumer-Group-消费者组" class="headerlink" title="9.Consumer Group 消费者组"></a>9.Consumer Group 消费者组</h3><p>如果生产者生产消息的速率过快，会造成消息在 Broker 的堆积，影响 Broker 的性能</p>
<p>怎么提升消息的消费速率呢？增加消费者的数量</p>
<p>但是这么多消费者，怎么知道大家是不是消费的同一个 Topic 呢？</p>
<p>所以引入了一个 Consumer Group 消费组的概念，在代码中通过 group id 来 配置</p>
<p>消费同一个 Topic 的消费者不一定是同一个组，只有 group id 相同的消费者才是同一个消费者组</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-consumerGroup.png" alt="image-20230413131624977" style="zoom:35%;" />

<p>注意:同一个 Group 中的消费者，不能消费相同的Partition——Partition 要在消费者之间分配</p>
<p>此时就会出现一个问题，即消费者比Partition多怎么办？比它少又怎么办？</p>
<ul>
<li>消费者比 Partition 少，一个消费者可能消费多个 Partition</li>
<li>消费者比 Partition 多，肯定有消费者没有 Partition 可以消费，不会出现一个 Group 里面的消费者消费同一个 Partition 的情况</li>
</ul>
<p>因此，如果想要同时消费同一个 Partition 的消息，那么需要其他的组来消费</p>
<h3 id="10-Consumer-Offset"><a href="#10-Consumer-Offset" class="headerlink" title="10.Consumer Offset"></a>10.Consumer Offset</h3><p>上面提到了，Partition里面的消息都是顺序写入，且读取之后不会被删除，那么消费者如何确定自己每次消费时的位置呢？</p>
<p>在Kafka当中，由于消息是有序存储，因此可以对消息进行<strong>编号</strong>，用来标识一条唯一的消息</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-sortNum.png" alt="image-20230413132604003" style="zoom:40%;" />

<p>而这个编号，就可以确定每个Partition当中消息的位置，称为偏移量（Offset）</p>
<p>Offset 记录着下一条将要发送给 Consumer 的消息的序号</p>
<p>这个消费者跟 Partition 之间的偏移量没有保存在 ZK，而是直接保存在服务端</p>
<blockquote>
<p>后面会详细介绍offset更新策略，此处点到为止</p>
</blockquote>
<p>此处，再回顾一下架构图就很清晰了</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka01.png" alt="在这里插入图片描述" style="zoom:38%;" />

<p>首先，这里有 3 台 Broker<br>有两个 Topic：Topic0 和 Topic1<br>Topic0 有 2 个分区：Partition0 和 Partition1，每个分区一共 3 个副本</p>
<p>Topic1 只有 1 个分区：Partition0，每个分区一共 3 个副本</p>
<p>图中红色字体的副本代表是 Leader，黑色字体的副本代表是 Follower</p>
<p>绿色的线代表是数据同步</p>
<p>蓝色的线是写消息，橙色的线是读消息，都是针对 Leader 节点</p>
<p>有两个消费者组，第一个消费者组，消费了 Topic0 的两个分区</p>
<p>第二个消费者组</p>
<ul>
<li>Consumer0：既消费消费 Topic0的 Partition0，还消费 Topic1 的 Partition0</li>
<li>Consumer1：消费 Partition0 的 Partition1</li>
<li>Consumer2：没有 Partition 可以消费</li>
</ul>
<p>为什么Consumer2没有Partition可以消费呢？</p>
<p>因为总共就三个Partition，已经被前两个相同group id的消费者消费掉了，它就只能闲着了</p>
<h2 id="三、对比"><a href="#三、对比" class="headerlink" title="三、对比"></a>三、对比</h2><h3 id="1-Kafka特性"><a href="#1-Kafka特性" class="headerlink" title="1.Kafka特性"></a>1.Kafka特性</h3><p>因为 kafka 是用来解决数据流的传输的问题的，所以它有这些特性: </p>
<ul>
<li><strong>高吞吐、低延迟：</strong>Kakfa 最大的特点就是收发消息非常快，Kafka 每秒可以处 理几十万条消息，它的最低延迟只有几毫秒</li>
<li><strong>高伸缩性：</strong>如果可以通过增加分区 Partition 来实现扩容。不同的分区可以在不同的 Broker 中。通过 ZK 来管理 Broker 实现扩展，ZK 管理 Consumer 可以实现负载</li>
<li><strong>持久性、可靠性：</strong>Kafka 能够允许数据的持久化存储，消息被持久化到磁盘， 并支持数据备份防止数据丢失</li>
<li><strong>容错性：</strong>允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作</li>
<li><strong>高并发：</strong>支持数千个客户端同时读写</li>
</ul>
<h3 id="2-Kafka与RabbitMQ对比"><a href="#2-Kafka与RabbitMQ对比" class="headerlink" title="2.Kafka与RabbitMQ对比"></a>2.Kafka与RabbitMQ对比</h3><p>Kafka 和 RabbitMQ 的主要区别</p>
<ul>
<li>产品侧重：Kafka:流式消息处理、消息引擎;RabbitMQ:消息代理</li>
<li>性能：Kafka 有更高的吞吐量。RabbitMQ 主要是 Push，Kafka 只有 Pull</li>
<li>消息顺序：分区里面的消息是有序的，同一个 Consumer Group 里面的一个消费者只能消费一个 Partition，能保证消息的顺序性</li>
<li>消息的路由和分发：RabbitMQ 更加灵活</li>
<li>延迟消息、死信队列：RabbitMQ 支持</li>
<li>消息的留存：Kafka 消费完之后消息会留存，RabbitMQ 消费完就会删除。 Kafka可以设置 retention，清理消息</li>
</ul>
<p>优先选择 RabbitMQ 的情况:</p>
<p>高级灵活的路由规则;</p>
<ul>
<li>消息时序控制(控制消息过期或者消息延迟);</li>
<li>高级的容错处理能力，在消费者更有可能处理消息不成功的情景中(瞬时或者持久)</li>
<li>更简单的消费者实现</li>
</ul>
<p>优先选择 Kafka 的情况</p>
<ul>
<li>严格的消息顺序</li>
<li>延长消息留存时间，包括过去消息重放的可能</li>
<li>传统解决方案无法满足的高伸缩能力</li>
</ul>
<h2 id="四、生产者详解"><a href="#四、生产者详解" class="headerlink" title="四、生产者详解"></a>四、生产者详解</h2><h3 id="1-幂等性"><a href="#1-幂等性" class="headerlink" title="1.幂等性"></a>1.幂等性</h3><p>在RabbitMQ当中，消息的幂等性是依赖全局唯一ID来实现的，需要在消费端实现</p>
<p>在Kafka当中，把这个任务交给了Broker，不再交给消费者解决</p>
<p>但是无论交给谁解决，肯定都会有一个全局唯一ID来标识消息，在Kafka中，这个ID可以通过以下配置产生</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(<span class="string">&quot;enable.idempotence&quot;</span>, <span class="literal">true</span>);</span><br></pre></td></tr></table></figure>

<p><code>enable.idempotence</code> 设置成 true 后，Producer 自动升级成幂等性 Producer，Kafka 会自动去重，如何实现的？</p>
<p>主要依赖两个重要机制：</p>
<ul>
<li><p>PID(Producer ID)：幂等性的生产者每个客户端都有一个唯一的编号</p>
</li>
<li><p>sequence number：幂等性的生产者发送的每条消息都会带相应的 sequence</p>
<p>number</p>
</li>
</ul>
<blockquote>
<p>Server端就是根据这个sequence number值来判断数据是否重复。如果说发现 sequence number 比服务端已经记录的值要小，那肯定是出现消息重复了</p>
</blockquote>
<p>但是，这个 sequence number 并不是全局有序的，它不能保证所有时间上的幂等</p>
<p>所以，它的作用范围是有限的</p>
<ul>
<li>只能保证单分区上的幂等性，即一个幂等性 Producer 能够保证<strong>某个主题的一 个分区</strong>上不出现重复消息</li>
<li>只能实现单会话上的幂等性，这里的会话指的是 Producer 进程的一次运行，当重启了 Producer 进程之后，幂等性不保证</li>
</ul>
<blockquote>
<p>这个也很容易理解，也就说不允许生产者在一次会话中向同一个 patition 发送相同的消息</p>
</blockquote>
<h3 id="2-生产者事务"><a href="#2-生产者事务" class="headerlink" title="2.生产者事务"></a>2.生产者事务</h3><p>生产者事务是 Kafka 2017 年 0.11.0.0 引入的新特性，通过事务，Kafka 可以保证跨生产者会话的消息<strong>幂等</strong>发送</p>
<p>为什么需要引入事务的特性呢？什么时候才需要开启事务？</p>
<p>有几种情况:</p>
<ul>
<li>假设只有 1 个 Broker，1 个 Topic 的分区只有 1 个副本，如果要发送多条消息，想要让这些消息全部成功或者全部失败，怎么办? </li>
<li>更加复杂的情况，如果生产者发送消息到多个 Topic 或者多个 Partition， 它们有可能分布在不同的服务器上，需要它们全部发送成功或者全部发送失败，应该怎么办?</li>
<li>还有一种情况，就是消费者和生产者在同一块代码中（consume-process-produce），从上游接收消息，经过处理后发给下游，这个时候 要保证接收消息和发送消息同时成功</li>
</ul>
<p>在SpringBoot当中，可以使用事务注解@Transaction开启（声明式事务）</p>
<p>那么它的分布式事务如何实现呢？下面是实现核心思想</p>
<ul>
<li>因为生产者的消息可能会跨分区，所以这里的事务是属于<strong>分布式事务</strong>。分布式事务的实现方式有很多，Kafka 选择了最常见的两阶段提交(2PC)：<strong>如果大家都可以 commit，那么就 commit，否则 abort</strong></li>
<li>既然是 2PC，必须要有一个协调者的角色，叫做 <strong>Transaction Coordinator</strong></li>
<li>事务管理必须要有事务日志，来记录事务的状态，以便 Coordinator 在意外挂掉之后继续处理原来的事务。跟消费者 Offset 的存储一样，Kafka 使用一个特殊的topic__transaction_state 来记录事务状态</li>
<li>如果生产者挂了，事务要在重启后可以继续处理，接着之前未处理完的事务， 或者在其他机器上处理，必须要有一个唯一的 ID，这个就是 <strong>transaction.id</strong>，这里可以使用 UUID。配置了 transaction.id，则此时 enable.idempotence 会被设置为 true (事务实现的前提是幂等性)。事务 ID 相同的生产者，可以接着处理原来的事务</li>
</ul>
<p>看下图事务代码实现流程</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-transaction.png" alt="image-20230413141041133" style="zoom:25%;" />

<p>步骤描述:</p>
<ul>
<li>A:生产者通过 initTransactions API 向 Coordinator 注册事务 ID</li>
<li>B:Coordinator 记录事务日志</li>
<li>C:生产者把消息写入目标分区</li>
<li>D:分区和 Coordinator 的交互，当事务完成以后，消息的状态应该是已提交，这样消费者才可以消费到</li>
</ul>
<h3 id="3-生产者原理"><a href="#3-生产者原理" class="headerlink" title="3.生产者原理"></a>3.生产者原理</h3><h4 id="3-1-生产者消费发送流程"><a href="#3-1-生产者消费发送流程" class="headerlink" title="3.1 生产者消费发送流程"></a>3.1 生产者消费发送流程</h4><p>消息发送的整体流程，生产端主要由两个线程协调运行</p>
<p>这两条线程分别为 main 线程和 Sender 线程(发送线程)</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-producer.png" alt="image-20230413140934823" style="zoom:30%;" />

<p>KafkaProducer就是代码中实现的生产者，它创建了一个Sender对象，启动了一个IO线程用于发送</p>
<h4 id="3-2-拦截器"><a href="#3-2-拦截器" class="headerlink" title="3.2 拦截器"></a>3.2 拦截器</h4><p>拦截器是执行在KafkaProducer之后的代码，它又有什么作用呢？</p>
<p>拦截器的作用是实现消息的定制化</p>
<blockquote>
<p>类似于:Spring Interceptor 、MyBatis 的 插件、Quartz 的监听器</p>
</blockquote>
<p>可以在生产者的属性中指定多个拦截器，形成拦截器链</p>
<p>举个例子，假设发送消息的时候要扣钱，发一条消息 1 分钱(我把这个功能叫做 按量付费)，就可以用拦截器实现</p>
<h4 id="3-3-序列化"><a href="#3-3-序列化" class="headerlink" title="3.3 序列化"></a>3.3 序列化</h4><p>消息发送都是需要序列化的，因此在调用 send 方法以后，第二步是利用指定的工具对 key 和 value 进行序列化</p>
<p>Kafka自带许多常见数据类型的序列化工具，此外也可以使用如 Avro、JSON、Thrift、Protobuf 等， 或者使用自定义类型的序列化器来实现，实现 Serializer 接口即可</p>
<h4 id="3-4-路由指定"><a href="#3-4-路由指定" class="headerlink" title="3.4 路由指定"></a>3.4 路由指定</h4><p>即一个消息要发送到哪个partition呢？在代码中是这样实现的</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> partition(record, serializedKey, serializedValue, cluster);</span><br></pre></td></tr></table></figure>

<p>它返回的是一个分区编号，从0开始</p>
<p>首先我们分一下，有四种情况: </p>
<ul>
<li>指定了 partition</li>
<li>没有指定 partition，自定义了分区器</li>
<li>没有指定 partition，没有自定义分区器，但是 key 不为空</li>
<li>没有指定 partition，没有自定义分区器，但是 key 是空的</li>
</ul>
<p>下面分别讨论这四种情况</p>
<p><strong>1）指定了 partition</strong></p>
<p>指定 partition 的情况下，直接将指定的值直接作为partiton值</p>
<p><strong>2）没有指定 partition，自定义了分区器</strong></p>
<p>自定义分区器，将使用自定义的分区器算法选择分区，比如 SimplePartitioner， 用 ProducerAutoPartition 指定，发送消息</p>
<p><strong>3）没有指定 partition，没有自定义分区器，但是 key 不为空</strong></p>
<blockquote>
<p>根据key，使用默认分区器</p>
</blockquote>
<p>没有指定 partition 值但有 key 的情况下，使用默认分区器 DefaultPartitioner，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值</p>
<p><strong>4）没有指定 partition，没有自定义分区器，但是 key 是空的</strong></p>
<blockquote>
<p>差不多就是随机了</p>
</blockquote>
<p>既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数(后面每次调用在这个整数上自增)，将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法</p>
<h4 id="3-5-消息累加器"><a href="#3-5-消息累加器" class="headerlink" title="3.5 消息累加器"></a>3.5 消息累加器</h4><p>事实上，在选择分区之后并没有直接发送消息，而是将消息放入了消息累加器</p>
<p>本质上是一个ConcurrentMap</p>
<p>一个partition一个batch，batch满了之后就会唤醒Sender线程发送消息，减少IO</p>
<h3 id="4-可靠性保证"><a href="#4-可靠性保证" class="headerlink" title="4.可靠性保证"></a>4.可靠性保证</h3><h4 id="4-1-服务器响应策略"><a href="#4-1-服务器响应策略" class="headerlink" title="4.1 服务器响应策略"></a>4.1 服务器响应策略</h4><p>在RabbitMQ当中，生产者发送完消息，需要依靠交换机回传的ack来确定消息投递成功，那么在kafka当中，是如何保证消息发送成功的呢？</p>
<p>首先要确定，服务端什么时候才算接收成功呢？</p>
<p>因为消息是存储在不同的Partition里面的，所以是<strong>写入到Partition之后响应生产者</strong></p>
<p>但是kafka的Partition是有副本存在的，所以是要写入一定量副本才可以响应</p>
<p>那么写入多少呢？这里主要有两种策略</p>
<p>第一种是需要<strong>有半数以上的 Follower 节点完成同步</strong>，这样的话客户端等待的时间就短一些，延迟低（为什么通常来说我们部署节点都是奇数？）</p>
<p>第二种需要<strong>所有的 Follower 全部完成同步</strong>，才发送 ACK 给客户端，延迟相对来说高一些，但是节点挂掉的影响相对来说小一些，因为所有的节点数据都是完整的</p>
<p>那么kafka选择的是哪一种呢？</p>
<p>kafka选择的是第二种，因为安全，而且网络延迟对kafka影响不大</p>
<h4 id="4-2-ISR"><a href="#4-2-ISR" class="headerlink" title="4.2 ISR"></a>4.2 ISR</h4><p>如果直接采用第二种思路，不考虑网络延迟，有没有别的问题呢？</p>
<p>假设 Leader 收到数据，所有 Follower 都开始同步数据，但是有一个 Follower 出了问题，没有办法从 Leader 同步数据。按照这个规则，Leader 就要一直等待，无法发送 ACK，可以说成为了害群之马</p>
<p>所以我们的规则就不能那么粗暴了，把规则改一下，不是所有的 follower 都有权利让我等待，而是只有那些正常工作的 follower 同步数据的时候我才会等待</p>
<p>我们应该把那些正常和 leader 保持同步的 replica 维护起来，放到一个动态 set 里面，这个就叫做 in-sync replica set(ISR)。现在只要 ISR 里面的 follower 同步完数据之后，我就给客户端发送 ACK</p>
<p>如果一个 Follower 长时间不同步数据，就要从 ISR 剔除，默认30s</p>
<p>而如果Leader长时间没有同步，那么就表示挂掉了，需要从ISR里重新选举Leader</p>
<h4 id="4-3-ACK应答机制"><a href="#4-3-ACK应答机制" class="headerlink" title="4.3 ACK应答机制"></a>4.3 ACK应答机制</h4><p>上面的方案很安全，但是有时候，我不想要很安全的数据传输，我就要快，那怎么实现呢？</p>
<p>Kafka 为客户端提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择相应的配置，配置方式如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pros.put(<span class="string">&quot;acks&quot;</span>,<span class="string">&quot;1&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>下面介绍三个参数含义，假设topic 的 partition0 有三个副本</p>
<p>acks&#x3D;0：producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据</p>
<p>acks&#x3D;1(默认)：producer 等待 broker 的 ack，partition 的<strong>leader落盘成功</strong>后返回 ack，如果在 follower 同步成功之前 leader 故障，那么将会丢失数据</p>
<p>acks&#x3D;-1(all)：producer 等待 broker 的 ack，partition 的 leader 和 follower <strong>全部落盘成功</strong>后才返回 ack</p>
<blockquote>
<p>acks&#x3D;-1(all) 这种方案是完美的吗?有没有可能出问题？</p>
<p>如果在 follower 同步完成后，broker 发送 ack 之前，leader 发生故障，没有给</p>
<p>生产者发送 ACK，那么会造成数据重复<br>在这种情况下， 把 reties 设置成 0(不重发)，才不会重复</p>
</blockquote>
<p>三种机制，性能依次递减 (producer 吞吐量降低)，数据健壮性则依次递增。我们可以根据业务场景使用不同的参数</p>
<h2 id="五、Broker详解"><a href="#五、Broker详解" class="headerlink" title="五、Broker详解"></a>五、Broker详解</h2><h3 id="1-存储原理"><a href="#1-存储原理" class="headerlink" title="1.存储原理"></a>1.存储原理</h3><h4 id="1-1-文件的存储结构"><a href="#1-1-文件的存储结构" class="headerlink" title="1.1 文件的存储结构"></a>1.1 文件的存储结构</h4><p>配置文件:config&#x2F;server.properties</p>
<p>logs.dir 配置</p>
<p>默认&#x2F;tmp&#x2F;kafka-logs</p>
<h4 id="1-2-Partition-分区"><a href="#1-2-Partition-分区" class="headerlink" title="1.2 Partition 分区"></a>1.2 Partition 分区</h4><p>为了实现横向扩展，把不同的数据存放在不同的 Broker 上，同时降低单台服务器的访问压力，我们把一个 Topic 中的数据分隔成多个 Partition<br> 一个 Partition 中的消息是有序的，顺序写入，但是全局不一定有序</p>
<blockquote>
<p>在服务器上，每个 Partition 都有一个物理目录，Topic 名字后面的数字标号即代表分区</p>
</blockquote>
<h4 id="1-3-Replica-副本"><a href="#1-3-Replica-副本" class="headerlink" title="1.3 Replica 副本"></a>1.3 Replica 副本</h4><p>为了提高分区的可靠性，Kafka 又设计了副本机制<br>创建 Topic 的时候，通过指定 replication-factor 确定 Topic 的副本数</p>
<blockquote>
<p>注意:副本数必须小于等于节点数，而不能大于 Broker 的数量，否则会报错</p>
<p>为什么可以等于节点数？因为Leader也称为副本</p>
</blockquote>
<p>这样就可以保证，绝对不会有一个分区的两个副本分布在同一个节点上，不然副本机制也失去了备份的意义了</p>
<p>这些所有的副本分为两种角色</p>
<ul>
<li>Leader：对外提供读写服务</li>
<li>Follower：唯一的任务就是从 Leader 异步拉取数据</li>
</ul>
<p>思考:为什么不能像 MySQL 一样实现读写分离，写操作都在 Leader 上，读操作<br>都在 Follower 上？</p>
<p>这个是设计思想的不同。读写都发生在 Leader 节点，就不存在读写分离带来的一<br>致性问题了，这个就叫做单调读一致性</p>
<h4 id="1-4-Leader"><a href="#1-4-Leader" class="headerlink" title="1.4 Leader"></a>1.4 Leader</h4><p>如果有多个副本，如何确定Leader呢？</p>
<p>在 Kafka 中，每个分区的初始 Leader 会在创建分区时进行选举。具体来说，当创建新的分区时，Kafka 会根据主题的分区副本分配策略，为该分区的每个副本节点分配一个角色，其中一个节点被选举为 Leader，其余节点则成为 Follower</p>
<p>在分配角色时，Kafka 会优先将 Leader 副本分配给与之前的 Leader 所在节点相同的节点，如果该节点不可用，则按照指定的分配策略从可用节点中选择一个节点作为 Leader</p>
<p>可知，Leader的选举与副本有很深的联系，因此这里先继续看副本</p>
<h4 id="1-5-副本在Broker的分布"><a href="#1-5-副本在Broker的分布" class="headerlink" title="1.5 副本在Broker的分布"></a>1.5 副本在Broker的分布</h4><p>副本在 Broker 的分布有什么规则吗？</p>
<p>事实上，副本的分布是由AdminUtils.scala 的 assignReplicasToBrokers 函数决定的</p>
<p>该函数规则如下：</p>
<ul>
<li>firt of all，副本因子不能大于 Broker 的个数</li>
<li><strong>第一个分区</strong>(编号为 0 的分区)的<strong>第一个副本</strong>放置位置是随机从 brokerList 选择的</li>
<li>其他分区的<strong>第一个副本</strong>放置位置相对于第 0 个分区依次往后移<ul>
<li>也就是说：如果我们有 5 个 Broker，5 个分区</li>
<li>假设第 1 个分区的第 1 个副本放在第四个 Broker 上</li>
<li>那么第 2 个分区的第 1 个副本将会放在第五个 Broker 上</li>
<li>第三个分区的第 1 个副本将会放在第一个 Broker 上</li>
<li>第四个分区的第 1 个副本将会放 在第二个 Broker 上，依次类推(蛇形走位)</li>
</ul>
</li>
<li>每个分区剩余的副本相对于第 1 个副本放置位置其实是由 nextReplicaShift决定的，而这个数也是随机产生的。</li>
</ul>
<p>用箭头解释如下图</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-replica-fir.png" alt="image-20230413153907038" style="zoom:30%;" />

<blockquote>
<p>但是要注意的是，这里说的都是每个分区第一个副本的放置位置</p>
<p>而这里就对应了上一个小结里说过的，leader怎么选出来的？</p>
<p>初始化时，leader就是第一个分区副本</p>
<p>假如leader挂了，那就由zk进行选举</p>
</blockquote>
<p>这样设计可以提高容灾能力，怎么讲?</p>
<p>在每个分区的第一个副本错开之后，一般第一个分区的第一个副本(按 Broker 编 号排序)都是 Leader，Leader 是错开的，不至于一挂影响太大</p>
<p>bin 目录下的 kafka-reassign-partitions.sh 可以根据 Broker 数量变化情况重新分配分区</p>
<p>这里再提另一个问题，一个分区是不是只有一个 文件呢？也就是说，消息日志文件是不是会无限地变大？</p>
<h4 id="1-6-Segment"><a href="#1-6-Segment" class="headerlink" title="1.6 Segment"></a>1.6 Segment</h4><p>为了防止 Log 不断追加导致文件过大，导致检索消息效率变低，一个 Partition 又被划分成多个 Segment 来组织数据(MySQL 也有 Segment 的逻辑概念，叶子节点就是数据段，非叶子节点就是索引段)</p>
<p>在磁盘上，每个 Segment 由一个 log 文件和 2 个 index 文件组成，且这三个文件成套出现</p>
<ul>
<li>log文件，是命名规则为00000000000000000000.log的文件</li>
<li>index文件两个<ul>
<li>00000000000000000000.index</li>
<li>00000000000000000000.timeindex</li>
</ul>
</li>
</ul>
<p><strong>1）日志文件</strong></p>
<p>在一个 Segment 文件里面，日志是追加写入的。如果满足一定条件，就会切分日志文件，产生一个新的 Segment</p>
<p>什么时候会触发 Segment 的切分呢？ </p>
<p>第一种是根据日志文件大小。当一个 Segment 写满以后，会创建一个新的 Segment，用最新的 Offset 作为名称</p>
<p>Segment 的默认大小是 1073741824 bytes(1G)，由参数<code>log.segment.bytes</code>控制</p>
<p>第二种是根据消息的最大时间戳，和当前系统时间戳的差值</p>
<p>有一个默认的参数，168 个小时(一周):</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log.roll.hours</span>=<span class="string">168</span></span><br></pre></td></tr></table></figure>

<p>意味着:如果服务器上次写入消息是一周之前，旧的 Segment 就不写了，现在要创建一个新的 Segment</p>
<blockquote>
<p>还可以从更加精细的时间单位进行控制，如果配置了毫秒级别的日志切分间隔， 会优先使用这个单位，否则就用小时的</p>
</blockquote>
<p>第三种是根据索引文件&#x2F;timeindex文件大小，默认是10M</p>
<p>这种方法的含义就是说，如果索引写满了，那数据文件也要跟着一起拆分，不然会对不上</p>
<p><strong>2）索引文件</strong></p>
<p>由于一个 Segment 的文件里面可能存放很多消息，如果要根据 Offset 获取消息， 必须要有一种快速检索消息的机制——这个就是索引</p>
<p>在 Kafka 中设计了两种索引</p>
<ul>
<li>偏移量索引文件记录的是 Offset 和消息物理地址(在 Log 文件中的位置)的映射关系</li>
<li>时间戳索引文件记录的是时间戳和 Offset 的关系</li>
</ul>
<p>当然，内容是二进制的文件，不能以纯文本形式查看</p>
<p><strong>offset索引</strong></p>
<p>bin 目录下有 dumplog 工 具，通过查看最后 10 条 Offset 索引(160 服务器)如下</p>
<img src="https://static-2w2.pages.dev/post/mq-index.png" alt="image-20230413155501413" style="zoom:50%;" />

<p>为什么索引里面记录的 Offset 不是连续的呢?不应该是一条消息一条索引记录吗？</p>
<p>注意 Kafka 的索引并不是每一条消息都会建立索引</p>
<p>而是一种稀疏索引 Sparse Index(DB2 和 MongDB 中都有稀疏索引)</p>
<p>抽象图如下：</p>
<img src="/Users/zjyan/Library/Application Support/typora-user-images/image-20230413155621134.png" alt="image-20230413155621134" style="zoom:25%;" />

<p>所以问题就来了，这个稀疏索引到底有多稀疏？也就是说，隔几条消息才产生一 个索引记录？或者隔多久？或者隔多少大小的消息？</p>
<p> 实际上是用消息的大小来控制的，默认是 4KB:</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log.index.interval.bytes</span>=<span class="string">4096</span></span><br></pre></td></tr></table></figure>

<p> 只要写入的消息超过了 4KB，偏移量索引文件.index 和时间戳索引文 件.timeindex 就会增加一条索引记录(索引项)。</p>
<p>这个值设置越小，索引越密集，值设置越大，索引越稀疏</p>
<p>相对来说，越稠密的索引检索数据更快，但是会消耗更多的存储空间</p>
<p>越稀疏的索引占用存储空间越小，插入和删除时所需的维护开销也小，但是检索慢</p>
<p> Kafka 索引的时间复杂度为 O(log2n)+O(m)</p>
<p>n 是索引文件里索引的个数，m 为稀疏程度</p>
<p><strong>时间戳索引</strong></p>
<p>为什么会有时间戳索引文件呢？光有 Offset 索引还不够吗？会根据时间戳来查找消息？</p>
<p>首先，消息是必须要记录时间戳的</p>
<p>客户端封装的 ProducerRecord 和ConsumerRecord 都有一个 long timestamp 属性</p>
<p>为什么要记录时间戳呢? </p>
<ul>
<li>如果要基于时间切分日志文件，必须要记录时间戳</li>
<li>如果要基于时间清理消息，必须要记录时间戳</li>
</ul>
<p>好吧，既然都已经记录时间戳了，干脆设计一个时间戳索引，可以根据时间戳查询</p>
<p>注意时间戳有两种</p>
<p>一种是消息创建的时间戳</p>
<p>一种是消费在 Broker 追加写入的时间</p>
<p>到底用哪个时间呢？由一个参数来控制</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CreateTime / LogAppendTime</span></span><br><span class="line"><span class="attr">log.message.timestamp.type</span>=<span class="string">CreateTime </span></span><br></pre></td></tr></table></figure>

<p><strong>3）索引查询</strong></p>
<p>既然已经有索引了，那么如何利用索引来进行消息的检索呢？</p>
<p>比如要检索偏移量是 10000666 的消息</p>
<ul>
<li>消费的时候是能够确定分区的，所以第一步是找到在哪个 Segment 中。 Segment 文件是用 Base Offset 命名的，所以可以用二分法很快确定(找到名字不小 于 10000666 的 Segment)</li>
<li>这个 Segment 有对应的索引文件，它们是成套出现的。所以现在要在索引文 件中根据 Offset 找 Position</li>
<li>得到 Position 之后，到对应的 Log 文件开始查找 Offset，和消息的 Offset 进行比较，直到找到消息</li>
</ul>
<blockquote>
<p>思考一个比较刁钻的面试问题：为什么 Kafka 不用 B+Tree?</p>
<p>Kafka 是写多，查少，如果 Kafka 用 B+Tree，首先会出现大量的 B+Tree，大量插入数据带来的 B+Tree 的调整会非常消耗性能</p>
</blockquote>
<h4 id="1-7-总结"><a href="#1-7-总结" class="headerlink" title="1.7 总结"></a>1.7 总结</h4><p>总体topic结构如下：</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-topic2.png" alt="image-20230413160645706" style="zoom:25%;" />

<p>到此为止topic结构已经完全梳理清楚了，但是有个问题还是要提一下，那就是kafka消息消费之后并不会删除，但是也不可能把几年前的日志还保留着，那么会用什么策略保留消息呢？</p>
<blockquote>
<p>思考MySQL、redis的内存淘汰机制</p>
</blockquote>
<h3 id="2-消息保留和清理机制"><a href="#2-消息保留和清理机制" class="headerlink" title="2.消息保留和清理机制"></a>2.消息保留和清理机制</h3><blockquote>
<p>这里回忆一下MySQL和Redis的内存淘汰机制</p>
<p><a href="https://yzj.life/2023/01/07/MySql%EF%BC%88%E4%B8%80%EF%BC%89/#7%EF%BC%89LRU">MySQL</a>：使用LRU List，划分young区和old区，冷热分离</p>
<p><a href="https://yzj.life/2023/02/24/redis%E8%AF%A6%E8%A7%A3/#2-%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5">Redis</a>：多达七种淘汰策略</p>
</blockquote>
<p>Kafka里面提供了两种方式：一种是直接删除 Delete，一种是对日志进行压缩Compact</p>
<p>默认是直接删除</p>
<h4 id="2-1-删除策略"><a href="#2-1-删除策略" class="headerlink" title="2.1 删除策略"></a>2.1 删除策略</h4><p>删除是如何删除的呢？</p>
<p>第一种策略：日志数据较均匀时</p>
<p>默认是5min执行一次删除，删除范围是168小时之前的老数据，也就是定时删老数据</p>
<p>定时时间和删多久之前的数据，都可以配置</p>
<p>第二种策略：日志数据分布不均时（可能某一周特别大，另一周特别小）</p>
<p>根据日志大小删除，先删旧的消息，删到不超过这个大小为止</p>
<h4 id="2-2-压缩策略"><a href="#2-2-压缩策略" class="headerlink" title="2.2 压缩策略"></a>2.2 压缩策略</h4><p>问题：如果同一个 Key 重复写入多次，会存储多次还是会更新？</p>
<p>比如用来存储位移的这个特殊的 topic:__consumer_offsets</p>
<p>存储的是消费者 id 和 Partition 的 Offset 关系，消费者不断地消费消息 commit 的时候，是直接更新原来的 Offset，还是不断地写入新的 Offset？</p>
<p>肯定是存储多次，不然怎么能实现顺序写</p>
<p>当有了这些 Key 相同的 Value 不同的消息的时候，存储空间就被浪费了</p>
<p>压缩就是把相同的 Key 合并为最后一个 Value——前面的都被覆盖，这也和更新offset差不多</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-compress.png" alt="image-20230413162951198" style="zoom:50%;" />

<h3 id="3-高可用及选举策略"><a href="#3-高可用及选举策略" class="headerlink" title="3.高可用及选举策略"></a>3.高可用及选举策略</h3><h4 id="3-1-Controller-选举"><a href="#3-1-Controller-选举" class="headerlink" title="3.1 Controller 选举"></a>3.1 Controller 选举</h4><p>当Leader宕机后，怎么选出来新的Leader呢？</p>
<p>早期的kafka是使用zk来进行投票选举，但是这样存在一个弊端，如果分区和副本数量过多，所有的副本都直接进行选举的话，一旦某个出现节点的增减，就会造成大量的 Watch 事件被触发，ZK 的就会负载过重，不堪重负</p>
<p>后来kafka更改了策略，不是所有的 Repalica 都参与 Leader 选举，而是由其中的一个 Broker 统一来指挥， 这个 Broker 的角色就叫做 Controller(控制器)</p>
<p>就像 Redis Sentinel 的架构，执行故障转移的时候，必须要先从所有哨兵中选一个负责做故障转移的节点一样，Kafka 也要先从所有 Broker 中选出唯一的一个 Controller</p>
<p>所有的 Broker 会尝试在 Zookeeper 中创建临时节点&#x2F;controller，只有一个能创建成功（先到先得）</p>
<p>如果 Controller 挂掉了或者网络出现了问题，ZK 上的临时节点会消失，其他的 Broker 通过 Watch 监听到 Controller 下线的消息后，开始竞选新的 Controller，方法跟之前还是一样的，谁先在 ZK 里面写入一个&#x2F;controller 节点，谁就成为新的Controller，这个 Controller 就相当于选举委员会的主席</p>
<p>那么这时候，这个节点就肩负了更多的责任，具体如下</p>
<ul>
<li>监听 Broker 变化</li>
<li>监听 Topic 变化</li>
<li>监听 Partition 变化</li>
<li>获取和管理 Broker、Topic、Partition 的信息</li>
<li>管理 Partiontion 的主从信息</li>
</ul>
<h4 id="3-2-分区副本Leader选举"><a href="#3-2-分区副本Leader选举" class="headerlink" title="3.2 分区副本Leader选举"></a>3.2 分区副本Leader选举</h4><p>Controller 确定以后，就可以开始做分区选主的事情了(我们叫它选举委员会主席)</p>
<p>下面就是找候选人了。显然，每个 Replica 都想推荐自己，但是所有的 replica 都有竞选资格吗？并不是</p>
<p>这里要介绍几个概念</p>
<p>一个分区所有的副本，叫做 Assigned-Replicas(AR)——所有的皇太子 </p>
<p>这些所有的副本中，跟 Leader 数据保持一定程度同步的，叫做 In-Sync Replicas (ISR)——天天过来参加早会的，有希望继位的皇太子</p>
<p>跟 Leader 同步滞后过多的副本，叫做 Out-Sync-Replicas(OSR)——天天睡懒觉，不参加早会，没被皇帝放在眼里的皇太子</p>
<p>AR&#x3D;ISR+OSR</p>
<p>正常情况下 OSR 是空的，大家都正常同步，AR&#x3D;ISR</p>
<p>谁能够参加选举呢？肯定不是 AR，也不是 OSR，而是 ISR，而且这个 ISR 不是固定不变的，还是一个动态的列表</p>
<p>前面说过，如果同步延迟超过 30 秒，就踢出 ISR，进入 OSR；如果赶上来了，就加入ISR</p>
<p>默认情况下，当 Leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 Leader</p>
<p>如果 ISR 为空呢？——皇帝突然驾崩，太子们都还小，但是群龙不能无首</p>
<p>在这种情 况下，可以让 ISR 之外的副本参与选举。允许 ISR 之外的副本参与选举，叫做 Unclean Leader Election。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">unclean.leader.election.enable</span>=<span class="string">false</span></span><br></pre></td></tr></table></figure>

<p>把这个参数改成 true(一般情况不建议开启，会造成数据丢失)</p>
<p>那么这时候，候选人有了，举荐人也有了，选择谁呢？根据什么规则呢？</p>
<blockquote>
<p>分布式系统中常见的选举协议有哪些？</p>
<p>ZAB(ZK)、Raft(Redis Sentinel)(他们都是 Paxos 算法的变种)，它们的 思想归纳起来都是:先到先得、少数服从多数</p>
</blockquote>
<p>是 Kafka 没有用这些方法，而是用了一种自己实现的算法</p>
<p>为什么呢?比如 ZAB 这种协议，可能会出现脑裂（节点不能互通的时候，出现多个Leader）、惊群效应（大量 Watch 事件被触发）</p>
<p>那么kafka使用的是什么算法呢？</p>
<p>在这篇文章中: <a target="_blank" rel="noopener" href="https://kafka.apache.org/documentation/#design_replicatedlog">https://kafka.apache.org/documentation/#design_replicatedlog</a><br>提到 Kafka 的选举实现，最相近的是微软的 PacificA 算法</p>
<p>在这种算法中，默认是让 ISR 中第一个 Replica 变成 Leader</p>
<p>比如 ISR 是 1、5、9，优先让1成为 Leader——这个跟中国古代皇帝传位是一样的，优先传给皇长子</p>
<h3 id="4-数据同步原理及故障处理"><a href="#4-数据同步原理及故障处理" class="headerlink" title="4.数据同步原理及故障处理"></a>4.数据同步原理及故障处理</h3><h4 id="4-1-主从同步"><a href="#4-1-主从同步" class="headerlink" title="4.1 主从同步"></a>4.1 主从同步</h4><p>在拥有一个确定的Leader之后，客户端的读写只能操作Leader节点，Follower需要向Leader同步数据</p>
<p>那么，由于不同的 Raplica 的 Offset 是不一样的，同步到底怎么同步呢？</p>
<p>先说下几个概念：</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-lfo.png" alt="image-20230413164637945" style="zoom:45%;" />

<p><strong>LEO</strong>（Log End Offset）：下一条等待写入的消息的 Offset(最新的 Offset + 1)</p>
<p>图中分别是 9、8、6</p>
<p><strong>HW</strong>（High Watermark）：ISR 中最小的 LEO，Leader会管理所有 ISR 中最小的LEO作为HW，目前是 6</p>
<p>实际上，<strong>Consumer 最多只能消费到 HW 之前的位置(消费到 Offset 5 的消息)</strong></p>
<p>也就是说，其他的副本没有同步过去的消息，是不能被消费的</p>
<p>为什么要这样设计呢？</p>
<p>如果在同步成功之前就被消费了，Consumer Group 的 Offset 会偏大，如果 Leader 崩溃，中间会缺失消息</p>
<p>所以这是怎么消费的，那么到底怎么同步呢？先看一下过程</p>
<p>Follower1 同步了 1 条消息，Follower2 同步了 2 条消息，此时 HW 推进了 2， 变成 8</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-leo2.png" alt="image-20230413165300926" style="zoom:45%;" />

<p>Follower1 同步了 0 条消息，Follower2 同步了 1 条消息。此时 HW 推进了 1， 变成 9</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-leo3.png" alt="image-20230413165409577" style="zoom:45%;" />

<p>LEO 和 HW 重叠，所有的消息都可以消费了</p>
<p>这里，我们关注一下，从节点怎么跟主节点保持同步？</p>
<ul>
<li>Follower 节点会向 Leader 发送一个 fetch 请求，Leader 向 Follower 发送数据后，既需要更新 Follower 的 LEO</li>
<li>Follower 接收到数据响应后，依次写入消息并且更新 LEO</li>
<li>Leader 更新 HW(ISR 最小的 LEO)</li>
</ul>
<p>Kafka 设计了独特的 ISR 复制，可以在保障数据一致性情况下又可提供高吞吐量</p>
<h4 id="4-2-Replica-故障处理"><a href="#4-2-Replica-故障处理" class="headerlink" title="4.2 Replica 故障处理"></a>4.2 Replica 故障处理</h4><p><strong>Follower故障</strong></p>
<p>首先 Follower 发生故障，会被<strong>先踢出 ISR</strong>——无法同步了</p>
<p>Follower 恢复之后，从哪里开始同步数据呢？</p>
<p>假设第 1 个 Replica 宕机（中间这个）</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-lfo.png" alt="image-20230413164637945" style="zoom:45%;" />

<p>恢复以后，首先根据之前记录的 HW(6)，把高于 HW 的消息截掉(6、7)</p>
<p>然后向 Leader 同步消息，追上 leader 之后(30 秒)，重新加入 ISR</p>
<p><strong>Leader故障</strong></p>
<p>假设图中 Leader 发生故障。<br>首先选一个 Leader，因为 Replica 1（中间这个）优先，它成为 Leader</p>
<p>为了保证数据一致，其他的 Follower 需要把高于 HW 的消息截取掉（这里没有消息需要截取）</p>
<p>然后 Replica2 同步数据</p>
<p>注意：这种机制只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复</p>
<h2 id="六、Consumer详解"><a href="#六、Consumer详解" class="headerlink" title="六、Consumer详解"></a>六、Consumer详解</h2><h3 id="1-消费者Offet维护原理"><a href="#1-消费者Offet维护原理" class="headerlink" title="1.消费者Offet维护原理"></a>1.消费者Offet维护原理</h3><h4 id="1-1-继续消费的Offet"><a href="#1-1-继续消费的Offet" class="headerlink" title="1.1 继续消费的Offet"></a>1.1 继续消费的Offet</h4><p>已知，在 Partition 中，消息是不会删除的，所以才可以追加写入，写入的消息连续有序的</p>
<p>这种特性决定了 Kafka 可以消费历史消息，而且按照消息的顺序消费指定消息， 而不是只能消费队头的消息</p>
<p>正常情况下，我们希望消费没有被消费过的数据，而且是从最先发送（序号小的）的开始消费（这样才是有序和公平的）</p>
<blockquote>
<p>首先要明确，一个partition的消费是按照<strong>消费者组</strong>为单位来消费的，一个消费者组里面的消费者不能重复消费同一个partition</p>
</blockquote>
<p>对于一个 Partition，消费者组怎么才能做到接着上次消费的位置（Offset）继续消费呢？肯定要把这个对应关系<strong>保存</strong>起来，下次消费的时候查找一下</p>
<p>那么它应该存在哪里呢？必然不可能存在消费者本地，因为所有消费者都可以使用这个 Consumer Group id，放到本地做不到同一维护的，因此要放在服务端</p>
<p>早期的kafka，是把消费者组和 Partition 的 Offset 直接维护在 ZK 中，但是因为读写的性能消耗太大了</p>
<p>后来就放在一个特殊的 Topic 中，名字叫__consumer_offsets， 默认有 50 个分区(offsets.topic.num.partitions 默认是 50)，每个分区默认一个 Replication</p>
<p>那么这个特殊的Topoc是怎么存储消费者组和分区之间的偏移量（offset）的呢？</p>
<p>在这个特殊的Topic里面，主要存储两种对象</p>
<ul>
<li>GroupMetadata：保存了消费者组中各个消费者的信息(每个消费者有编号)</li>
<li>OffsetAndMetadata：保存了消费者组和各个 partition 的 offset 位移信息元数据</li>
</ul>
<p>即一个保存消费者组和组里消费者的信息，一个保存消费者组和分片之间位移</p>
<p>这样当有新的消费时，可以直接根据这个特殊的Topic找到Offset，从而继续消费</p>
<h4 id="1-2-初始消费的Offet"><a href="#1-2-初始消费的Offet" class="headerlink" title="1.2 初始消费的Offet"></a>1.2 初始消费的Offet</h4><p>那么如果有一个新消费组的，那此时这个特殊的Topic里面没有保存Offset，如何开始消费呢？</p>
<p>消费者的代码中有一个参数，用来控制如果找不到偏移量的时候从哪里开始消费</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">auto.offset.reset</span></span><br></pre></td></tr></table></figure>

<ul>
<li>默认值是 latest，也就是从最新的消息(最后发送的)开始消费的，此时<strong>历史消费是不能消费的</strong></li>
<li>earliest 代表从最早的(最先发送的)消息开始消费，可以消费到历史消息</li>
<li>none，如果 Consumer Group 在服务端找不到 offset 会报错</li>
</ul>
<h4 id="1-3-更新offset"><a href="#1-3-更新offset" class="headerlink" title="1.3 更新offset"></a>1.3 更新offset</h4><p>何时更新offset呢？</p>
<p>offset的更新是由消费者发出一个commit动作之后，才会在broker里面更新</p>
<p>那么这个commit动作什么时候发生呢？</p>
<p>同样，这个动作可以手动提交，也可以自动提交，默认是自动提交true</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提交方式</span></span><br><span class="line"><span class="attr">enable.auto.commit</span></span><br><span class="line"><span class="comment"># 自动提交频率</span></span><br><span class="line"><span class="attr">auto.commit.interval.ms</span></span><br></pre></td></tr></table></figure>

<p>如果想在消息消费完毕之后再更新，就要改为手动提交，两种提交方式</p>
<ul>
<li>consumer.commitSync()的手动同步提交</li>
<li>consumer.commitAsync()手动异步提交</li>
</ul>
<blockquote>
<p>如果不提交或者提交失败，Broker 的 Offset 不会更新，消费者组下次消费的时候会消费到重复的消息</p>
</blockquote>
<h3 id="2-消费者与分区的关系"><a href="#2-消费者与分区的关系" class="headerlink" title="2.消费者与分区的关系"></a>2.消费者与分区的关系</h3><p>所谓消费者与分区之间的关系，就是消费者消费哪个分区的问题</p>
<p>这里主要有三种情况</p>
<ul>
<li>消费者组里面的消费者数量等于Topic里面的分区partition数量</li>
<li>消费者组里面的消费者数量大于Topic里面的分区partition数量</li>
<li>消费者组里面的消费者数量小于Topic里面的分区partition数量</li>
</ul>
<p>但是要注意的是，消费者组和分区也可存在多对多的关系，唯一的限制就是同一个消费者组里面的消费者不能消费同一个Topic里面的分区两次</p>
<h4 id="2-1-消费策略"><a href="#2-1-消费策略" class="headerlink" title="2.1 消费策略"></a>2.1 消费策略</h4><p>这种情况下，必然有一些消费者是消费不到的，那么这些消费者是如何瓜分partition的呢？</p>
<p>默认策略是：RangeAssignor 范围分配</p>
<p>如下图</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-rangeAssignor.png" alt="image-20230413174714402" style="zoom:40%;" />

<p>在这种模式中，会将分区按照范围划分，最后一个消费者可能分到的分区会少一点</p>
<p>还有另外两种策略，分别是RoundRobinAssignor（轮询）和StickyAssignor（粘滞）</p>
<p>RoundRobinAssignor（轮询）：即消费者轮流获取分区，按照上图的结构</p>
<p>C1对应分区：P0、P2、P4</p>
<p>C2对应分区：P1、P3</p>
<p>StickyAssignor（粘滞）：这种策略复杂一点，但是相对来说均匀一点，每次结果可能不一样</p>
<p>原则</p>
<ul>
<li>分区的分配尽可能的均匀</li>
<li>分区的分配尽可能和上次分配保持相同</li>
</ul>
<p>具体来说，StickyAssignor 会先将每个 Partition 分配给一个消费者，然后将剩余的 Partition 逐个分配给消费者组中还未分配到该 Partition 的消费者，以确保每个消费者都能消费到一定数量的 Partition</p>
<h4 id="2-2-ReBalance-分区再均衡"><a href="#2-2-ReBalance-分区再均衡" class="headerlink" title="2.2 ReBalance 分区再均衡"></a>2.2 ReBalance 分区再均衡</h4><p>进入话题之前，先提一个问题，什么叫ReBalance？</p>
<p>ReBalance本质上是一种协议，规定了一个消费者组下的所有消费者如何达成一致来分配订阅Topic的每个分区</p>
<p>比如某Group有20个消费者，订阅了有100个分区的Topic，那这时候Kafka就会为每个消费者分配5个分区，这个过程就是ReBalance</p>
<p><strong>那么什么时候ReBalance呢？</strong></p>
<p>两种情况</p>
<ul>
<li>消费者组的消费者数量发生变化，比如新增了消费者，消费者关闭连接 —— 学生数量变多了</li>
<li>Topic 的分区数发生变更，新增或者减少 —— 座位数量发生了变化</li>
</ul>
<p>为了让分区分配尽量地均衡，这个时候会触发 ReBalance 机制</p>
<p><strong>那么谁来执行 ReBalance 和 Consumer Group 管理呢？</strong></p>
<p>Kafka 提供了一个角色：Coordinator来执行对于Consumer Group的管理</p>
<blockquote>
<p>Kafka早期版本中的 Coordinator 是依赖 Zookeeper 来实现的</p>
</blockquote>
<p>最新的版本中，Kafka 对Coordinator进行了改进，每个Consumer Group都会被分配一个 Coordinator 用于组管理和 Offser 管理</p>
<p>责任：这个Group内的Coordinator比原来承担了更多的责任，比如组成员管理、Offset 提交保护机制等</p>
<p>选取：Consumer Group中的第一个Consumer启动的时候，它会去和 Kafka 服务确定谁是它们组的 Coordinator</p>
<p>协调：然后Group内的所有成员都会和这个Coordinator进行协调通信</p>
<p>很显然，有了Coordinator这个设计，就不再需要Zookeeper了，性能上可以得到很大的提升</p>
<p>那么 Consumer Group 又是如何确定自己的 Coordinator 是谁的呢?？</p>
<p>其实非常简单，就是找到分区的Leader所在的Broker，就会被选定为Coordinator</p>
<h2 id="七、Kafka为什么那么快？"><a href="#七、Kafka为什么那么快？" class="headerlink" title="七、Kafka为什么那么快？"></a>七、Kafka为什么那么快？</h2><p>MQ 的消息存储有几种选择，一种是内存，比如 ZeroMQ，速度很快但是不可靠</p>
<p>一种是第三方的数据库，会产生额外的网络消耗，而且数据库出问题会影响存储</p>
<p>所以最常见的是把数据放在磁盘上存储</p>
<p>但是我们也都知道，磁盘的 I&#x2F;O 是比较慢的，选择磁盘作为存储怎么实现高吞吐、低延迟、高性能呢？</p>
<p>总结起来，主要是 4 点:磁盘顺序 I&#x2F;O、索引机制、批量操作和压紧、零拷贝</p>
<h3 id="1-顺序索引"><a href="#1-顺序索引" class="headerlink" title="1.顺序索引"></a>1.顺序索引</h3><p>随机 I&#x2F;O 就是:读写的多条数据在磁盘上是分散的，寻址会很耗时<br>顺序 I&#x2F;O 读写的数据在磁盘上是集中的，不需要重复寻址的过程</p>
<p>Kafka 的 Message 是不断追加到本地磁盘文件末尾的，而不是随机的写入，这使 得 Kafka 写入吞吐量得到了显著提升</p>
<p>内存 I&#x2F;O 是不是一定比磁盘 I&#x2F;O 快呢? <a target="_blank" rel="noopener" href="https://queue.acm.org/detail.cfm?id=1563874">https://queue.acm.org/detail.cfm?id=1563874</a></p>
<h3 id="2-索引"><a href="#2-索引" class="headerlink" title="2.索引"></a>2.索引</h3><p>参照上面第五节的1.6里面索引介绍</p>
<h3 id="3-批量读写和文件压缩"><a href="#3-批量读写和文件压缩" class="headerlink" title="3.批量读写和文件压缩"></a>3.批量读写和文件压缩</h3><p>批量读写和文件压缩 它把所有的消息都变成一个批量的文件，并且进行合理的批量压缩，减少网络 IO 损耗<br> <a target="_blank" rel="noopener" href="http://kafka.apache.org/documentation/#recordbatch">http://kafka.apache.org/documentation/#recordbatch</a></p>
<h3 id="4-零拷贝"><a href="#4-零拷贝" class="headerlink" title="4.零拷贝"></a>4.零拷贝</h3><p>首先介绍两个名次</p>
<p><strong>第一个是操作系统虚拟内存的内核空间和用户空间</strong></p>
<p> 操作系统的虚拟内存分成了两块，一部分是内核空间，一部分是用户空间</p>
<p>这样就可以避免用户进程直接操作内核，保证内核安全</p>
<p>进程在内核空间可以执行任意命令，调用系统的一切资源;在用户空间必须要通过一些系统接口才能向内核发出指令</p>
<p>因此，如果用户要从磁盘读取数据(比如 Kafka 消费消息)，必须先把数据从磁盘拷贝到内核缓冲区，然后在从内核缓冲区到用户缓冲区，最后才能返回给用户</p>
<p><strong>第二个是 DMA 拷贝</strong></p>
<p>没有 DMA 技术的时候，拷贝数据的事情需要 CPU 亲自去做，这个时候它没法干其他的事情，如果传输的数据量大那就有问题了</p>
<p>DMA 技术叫做直接内存访问(Direct Memory Access)，其实可以理解为 CPU给自己找了一个小弟帮它做数据搬运的事情</p>
<p>在进行 I&#x2F;O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，解放了 CPU 的双手(反正就是找了个 小弟)</p>
<p>理解了这两个东西之后，我们来看下传统的 I&#x2F;O 模型</p>
<img src="https://static-2w2.pages.dev/post/mw-kafka-amd.png" alt="image-20230413185050276" style="zoom:45%;" />

<p>比如 Kafka 要消费消息，比如要先把数据从磁盘拷贝到内核缓冲区，然后拷贝到用户缓冲区，再拷贝到 Socket 缓冲区，再拷贝到网卡设备</p>
<p>这里面发生了 4 次用户态 和内核态的切换和 4 次数据拷贝，2 次系统函数的调用(read、write)，这个过程是非常耗费时间的。怎么优化呢?</p>
<p>在 Linux 操作系统里面提供了一个 sendfile 函数，可以实现“零拷贝”。这个时候就不需要经过用户缓冲区了，直接把数据拷贝到网卡(这里画的是支持 SG-DMA 拷 贝的情况)</p>
<p>因为这个只有 DMA 拷贝，没有 CPU 拷贝，所以叫做“零拷贝”</p>
<p>零拷贝至少可以提高一倍的性能</p>
<img src="https://static-2w2.pages.dev/post/mq-kafka-zero-copy.png" alt="image-20230413185453985" style="zoom:45%;" />

<h2 id="八、总结—Kafka消息不丢失的配置"><a href="#八、总结—Kafka消息不丢失的配置" class="headerlink" title="八、总结—Kafka消息不丢失的配置"></a>八、总结—Kafka消息不丢失的配置</h2><p>1、Producer 端使用 producer.send(msg, callback)带有回调的 send 方法，而不是 producer.send(msg)方法。根据回调，一旦出现消息提交失败的情况，就可以有针对性地进行处理</p>
<p>2、设置 acks &#x3D; all。acks 是 Producer 的一个参数，代表“已提交”消息的定义。 如果设置成 all，则表明所有 Broker 都要接收到消息，该消息才算是“已提交”</p>
<p>3、设置 retries 为一个较大的值。同样是 Producer 的参数。当出现网络抖动时， 消息发送可能会失败，此时配置了 retries 的 Producer 能够自动重试发送消息，尽量 避免消息丢失</p>
<p>4、设置 unclean.leader.election.enable &#x3D; false。这是 Broker 端的参数，在 Kafka 版本迭代中社区也多次反复修改过他的默认值，之前比较具有争议。它控制哪些 Broker 有资格竞选分区的 Leader。如果一个 Broker 落后原先的 Leader 太多，那么它一旦成为新的 Leader，将会导致消息丢失。故一般都要将该参数设置成 false</p>
<p>5、设置 replication.factor &gt;&#x3D; 3。需要三个以上的副本</p>
<p>6、设置 min.insync.replicas &gt; 1。Broker 端参数，控制消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。在生产环境中不要使用默认值 1。确保 replication.factor &gt; min.insync.replicas。如果两者相等，那么 只要有一个副本离线，整个分区就无法正常工作了。推荐设置成 replication.factor &#x3D; min.insync.replicas + 1</p>
<p>7、确保消息消费完成再提交。Consumer 端有个参数 enable.auto.commit，最 好设置成 false，并自己来处理 Offset 的提交更新</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://yzj.life">zjyan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yzj.life/2023/04/04/MQ%E4%B9%8BKafka/">http://yzj.life/2023/04/04/MQ%E4%B9%8BKafka/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yzj.life" target="_blank">春</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MQ/">MQ</a><a class="post-meta__tags" href="/tags/Kafka/">Kafka</a><a class="post-meta__tags" href="/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列</a></div><div class="post_share"><div class="social-share" data-image="https://static-2w2.pages.dev/black/10.jpeg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/04/03/MQ%E4%B9%8BRocketMQ/"><img class="next-cover" src="https://static-2w2.pages.dev/black/4.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">消息中间件之RocketMQ</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/04/02/MQ%E4%B9%8BRabbitMQ/" title="消息中间件之RabbitMQ"><img class="cover" src="https://static-2w2.pages.dev/black/6.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-02</div><div class="title">消息中间件之RabbitMQ</div></div></a></div><div><a href="/2023/04/03/MQ%E4%B9%8BRocketMQ/" title="消息中间件之RocketMQ"><img class="cover" src="https://static-2w2.pages.dev/black/4.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-03</div><div class="title">消息中间件之RocketMQ</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://static-2w2.pages.dev/imgs/cx.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">zjyan</div><div class="author-info__description">爱我所爱，尽我所能</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhijie-yan"><i class="fab fa-github"></i><span>欢迎交流～</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://space.bilibili.com/304087712" target="_blank" title="Bilibili"><i class="fab fa-bilibili"></i></a><a class="social-icon" href="mailto:yan1255015228@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><center>我想你一定很忙<br>所以<br>只看前三个字就好了<br></center></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80"><span class="toc-text">一、引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%B5%B7%E6%BA%90"><span class="toc-text">1.起源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%BA%94%E7%94%A8"><span class="toc-text">2.应用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92-Messaging"><span class="toc-text">2.1 消息传递 Messaging</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E7%BD%91%E7%AB%99%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90"><span class="toc-text">2.2 网站行为分析</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-1-Website-Activity-Tracking-%E7%BD%91%E7%AB%99%E6%B4%BB%E5%8A%A8%E8%B7%9F%E8%B8%AA"><span class="toc-text">2.2.1 Website Activity Tracking 网站活动跟踪</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-2-Log-Aggregation-%E6%97%A5%E5%BF%97%E8%81%9A%E5%90%88"><span class="toc-text">2.2.2 Log Aggregation 日志聚合</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-3-Metrics-%E5%BA%94%E7%94%A8%E6%8C%87%E6%A0%87%E7%9B%91%E6%8E%A7"><span class="toc-text">2.2.3 Metrics 应用指标监控</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90-%E6%B5%81%E8%AE%A1%E7%AE%97"><span class="toc-text">2.3 数据集成+流计算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Kafka%E5%92%8CZK%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-text">3.Kafka和ZK的关系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-text">二、基础架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Broker"><span class="toc-text">1.Broker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%B6%88%E6%81%AF"><span class="toc-text">2.消息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-text">3.生产者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-text">4.消费者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Topic"><span class="toc-text">5.Topic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Partition-%E4%B8%8E-Cluster"><span class="toc-text">6.Partition 与 Cluster</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Partition-%E5%89%AF%E6%9C%AC-Replica-%E6%9C%BA%E5%88%B6"><span class="toc-text">7.Partition 副本 Replica 机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Segment"><span class="toc-text">8.Segment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Consumer-Group-%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84"><span class="toc-text">9.Consumer Group 消费者组</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Consumer-Offset"><span class="toc-text">10.Consumer Offset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AF%B9%E6%AF%94"><span class="toc-text">三、对比</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Kafka%E7%89%B9%E6%80%A7"><span class="toc-text">1.Kafka特性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Kafka%E4%B8%8ERabbitMQ%E5%AF%B9%E6%AF%94"><span class="toc-text">2.Kafka与RabbitMQ对比</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%94%9F%E4%BA%A7%E8%80%85%E8%AF%A6%E8%A7%A3"><span class="toc-text">四、生产者详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-text">1.幂等性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%94%9F%E4%BA%A7%E8%80%85%E4%BA%8B%E5%8A%A1"><span class="toc-text">2.生产者事务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%94%9F%E4%BA%A7%E8%80%85%E5%8E%9F%E7%90%86"><span class="toc-text">3.生产者原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="toc-text">3.1 生产者消费发送流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E6%8B%A6%E6%88%AA%E5%99%A8"><span class="toc-text">3.2 拦截器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-text">3.3 序列化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-%E8%B7%AF%E7%94%B1%E6%8C%87%E5%AE%9A"><span class="toc-text">3.4 路由指定</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-%E6%B6%88%E6%81%AF%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-text">3.5 消息累加器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BF%9D%E8%AF%81"><span class="toc-text">4.可靠性保证</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%93%8D%E5%BA%94%E7%AD%96%E7%95%A5"><span class="toc-text">4.1 服务器响应策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-ISR"><span class="toc-text">4.2 ISR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-ACK%E5%BA%94%E7%AD%94%E6%9C%BA%E5%88%B6"><span class="toc-text">4.3 ACK应答机制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81Broker%E8%AF%A6%E8%A7%A3"><span class="toc-text">五、Broker详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86"><span class="toc-text">1.存储原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E6%96%87%E4%BB%B6%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84"><span class="toc-text">1.1 文件的存储结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-Partition-%E5%88%86%E5%8C%BA"><span class="toc-text">1.2 Partition 分区</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-Replica-%E5%89%AF%E6%9C%AC"><span class="toc-text">1.3 Replica 副本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-Leader"><span class="toc-text">1.4 Leader</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-%E5%89%AF%E6%9C%AC%E5%9C%A8Broker%E7%9A%84%E5%88%86%E5%B8%83"><span class="toc-text">1.5 副本在Broker的分布</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-Segment"><span class="toc-text">1.6 Segment</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-7-%E6%80%BB%E7%BB%93"><span class="toc-text">1.7 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%B6%88%E6%81%AF%E4%BF%9D%E7%95%99%E5%92%8C%E6%B8%85%E7%90%86%E6%9C%BA%E5%88%B6"><span class="toc-text">2.消息保留和清理机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5"><span class="toc-text">2.1 删除策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E5%8E%8B%E7%BC%A9%E7%AD%96%E7%95%A5"><span class="toc-text">2.2 压缩策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%8A%E9%80%89%E4%B8%BE%E7%AD%96%E7%95%A5"><span class="toc-text">3.高可用及选举策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-Controller-%E9%80%89%E4%B8%BE"><span class="toc-text">3.1 Controller 选举</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%ACLeader%E9%80%89%E4%B8%BE"><span class="toc-text">3.2 分区副本Leader选举</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86%E5%8F%8A%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86"><span class="toc-text">4.数据同步原理及故障处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5"><span class="toc-text">4.1 主从同步</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-Replica-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86"><span class="toc-text">4.2 Replica 故障处理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81Consumer%E8%AF%A6%E8%A7%A3"><span class="toc-text">六、Consumer详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%B6%88%E8%B4%B9%E8%80%85Offet%E7%BB%B4%E6%8A%A4%E5%8E%9F%E7%90%86"><span class="toc-text">1.消费者Offet维护原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E7%BB%A7%E7%BB%AD%E6%B6%88%E8%B4%B9%E7%9A%84Offet"><span class="toc-text">1.1 继续消费的Offet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E5%88%9D%E5%A7%8B%E6%B6%88%E8%B4%B9%E7%9A%84Offet"><span class="toc-text">1.2 初始消费的Offet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E6%9B%B4%E6%96%B0offset"><span class="toc-text">1.3 更新offset</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%8E%E5%88%86%E5%8C%BA%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-text">2.消费者与分区的关系</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E6%B6%88%E8%B4%B9%E7%AD%96%E7%95%A5"><span class="toc-text">2.1 消费策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-ReBalance-%E5%88%86%E5%8C%BA%E5%86%8D%E5%9D%87%E8%A1%A1"><span class="toc-text">2.2 ReBalance 分区再均衡</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81Kafka%E4%B8%BA%E4%BB%80%E4%B9%88%E9%82%A3%E4%B9%88%E5%BF%AB%EF%BC%9F"><span class="toc-text">七、Kafka为什么那么快？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E9%A1%BA%E5%BA%8F%E7%B4%A2%E5%BC%95"><span class="toc-text">1.顺序索引</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%B4%A2%E5%BC%95"><span class="toc-text">2.索引</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%89%B9%E9%87%8F%E8%AF%BB%E5%86%99%E5%92%8C%E6%96%87%E4%BB%B6%E5%8E%8B%E7%BC%A9"><span class="toc-text">3.批量读写和文件压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E9%9B%B6%E6%8B%B7%E8%B4%9D"><span class="toc-text">4.零拷贝</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%80%BB%E7%BB%93%E2%80%94Kafka%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1%E7%9A%84%E9%85%8D%E7%BD%AE"><span class="toc-text">八、总结—Kafka消息不丢失的配置</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/04/04/MQ%E4%B9%8BKafka/" title="消息中间件之Kafka"><img src="https://static-2w2.pages.dev/black/10.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="消息中间件之Kafka"/></a><div class="content"><a class="title" href="/2023/04/04/MQ%E4%B9%8BKafka/" title="消息中间件之Kafka">消息中间件之Kafka</a><time datetime="2023-04-04T11:45:59.000Z" title="发表于 2023-04-04 19:45:59">2023-04-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/03/MQ%E4%B9%8BRocketMQ/" title="消息中间件之RocketMQ"><img src="https://static-2w2.pages.dev/black/4.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="消息中间件之RocketMQ"/></a><div class="content"><a class="title" href="/2023/04/03/MQ%E4%B9%8BRocketMQ/" title="消息中间件之RocketMQ">消息中间件之RocketMQ</a><time datetime="2023-04-03T11:45:46.000Z" title="发表于 2023-04-03 19:45:46">2023-04-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/02/MQ%E4%B9%8BRabbitMQ/" title="消息中间件之RabbitMQ"><img src="https://static-2w2.pages.dev/black/6.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="消息中间件之RabbitMQ"/></a><div class="content"><a class="title" href="/2023/04/02/MQ%E4%B9%8BRabbitMQ/" title="消息中间件之RabbitMQ">消息中间件之RabbitMQ</a><time datetime="2023-04-02T11:45:31.000Z" title="发表于 2023-04-02 19:45:31">2023-04-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/31/Zookeeper/" title="Zookeeper"><img src="https://static-2w2.pages.dev/black/7.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Zookeeper"/></a><div class="content"><a class="title" href="/2023/03/31/Zookeeper/" title="Zookeeper">Zookeeper</a><time datetime="2023-03-31T01:29:00.000Z" title="发表于 2023-03-31 09:29:00">2023-03-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/26/ElasticSearch/" title="ElasticSearch"><img src="https://static-2w2.pages.dev/black/9.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ElasticSearch"/></a><div class="content"><a class="title" href="/2023/03/26/ElasticSearch/" title="ElasticSearch">ElasticSearch</a><time datetime="2023-03-26T01:40:06.000Z" title="发表于 2023-03-26 09:40:06">2023-03-26</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://static-2w2.pages.dev/black/10.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By zjyan</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadGiscus () {
  let nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'light'

  const config = Object.assign({
    src: 'https://giscus.app/client.js',
    'data-repo': 'zhijie-yan/Giscus',
    'data-repo-id': 'R_kgDOJSiR1Q',
    'data-category-id': 'DIC_kwDOJSiR1c4CVhQv',
    'data-mapping': 'pathname',
    'data-theme': nowTheme,
    'data-reactions-enabled': '1',
    crossorigin: 'anonymous',
    async: true
  },null)

  let ele = document.createElement('script')
  for (let key in config) {
    ele.setAttribute(key, config[key])
  }
  document.getElementById('giscus-wrap').insertAdjacentElement('afterbegin',ele)
}

function changeGiscusTheme () {
  const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'light'

  function sendMessage(message) {
    const iframe = document.querySelector('iframe.giscus-frame');
    if (!iframe) return;
    iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
  }

  sendMessage({
    setConfig: {
      theme: theme
    }
  });
}

if ('Giscus' === 'Giscus' || !false) {
  if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
  else loadGiscus()
} else {
  function loadOtherComment () {
    loadGiscus()
  }
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["link[rel=\"canonical\"]","meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"laft","width":150,"height":300},"mobile":{"show":true},"algolia":{"indexName":"hexo","applicationID":"T31GFL5LNX","apiKey":"180a4925ad0cc4ca01c1efcbe8b6ec59","fields":["title","content","url"],"debug":false,"concurrent":2},"log":false});</script></body></html>